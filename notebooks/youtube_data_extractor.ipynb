{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de62fd31",
   "metadata": {},
   "source": [
    "# Bowtie Youtube Channel Report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a1edd44",
   "metadata": {},
   "source": [
    "## Data Loading\n",
    "Load and examine the initial structure of the datasets required for this analysis.\n",
    "\n",
    "Data Extraction: Utilized the YouTube API to gather comprehensive data from three selected channels, including video metadata, view counts, likes, comments, and more."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b5fc8ba",
   "metadata": {},
   "source": [
    "# Extract\n",
    "## General Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a6e422f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import required packages\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from googleapiclient.discovery import build\n",
    "import mysql.connector\n",
    "from sqlalchemy import create_engine\n",
    "from datetime import datetime\n",
    "import re\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from gradio_client import Client\n",
    "from wordcloud import WordCloud\n",
    "from tqdm import tqdm\n",
    "import csv\n",
    "from googleapiclient.errors import HttpError\n",
    "import time\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "57e60fc0",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# # Get YouTube API key from .env \n",
    "youtube_api_key = os.getenv(\"youtube_api_key\")\n",
    "\n",
    "# # Get PostgreSQL connection details from .env\n",
    "# pg_host = os.getenv(\"pg_host\")\n",
    "# pg_port = os.getenv(\"pg_port\")\n",
    "# pg_user = os.getenv(\"pg_user\")\n",
    "# pg_password = os.getenv(\"pg_password\")\n",
    "# pg_database = os.getenv(\"pg_database\")\n",
    "\n",
    "# # Connect to local PostgreSQL database\n",
    "# connection = psycopg2.connect(\n",
    "#     host=pg_host,\n",
    "#     port=pg_port,\n",
    "#     user=pg_user,\n",
    "#     password=pg_password,\n",
    "#     database=pg_database\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "28def77a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the YouTube service object\n",
    "youtube = build(\"youtube\", \"v3\", developerKey=youtube_api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a317b5a",
   "metadata": {},
   "source": [
    "## Channel data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fc193e62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>channel_id</th>\n",
       "      <th>channel_name</th>\n",
       "      <th>views</th>\n",
       "      <th>total_videos</th>\n",
       "      <th>subscribers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>UCCjW9xzAsCSKIzDYCx8CuxA</td>\n",
       "      <td>智偉保險理財Talk</td>\n",
       "      <td>2806950</td>\n",
       "      <td>427</td>\n",
       "      <td>39400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>UCxQfqaw1i39eBQG1YJDbDkw</td>\n",
       "      <td>UTOPIA HK</td>\n",
       "      <td>620097</td>\n",
       "      <td>314</td>\n",
       "      <td>4170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>UCFfbH3zDLa47d4nfotQ349Q</td>\n",
       "      <td>投資最容易</td>\n",
       "      <td>3304042</td>\n",
       "      <td>394</td>\n",
       "      <td>34900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>UC7OUGIPx0HIB5HA2OSL-Zhg</td>\n",
       "      <td>MW Insurance Academe 保險為什麼</td>\n",
       "      <td>794169</td>\n",
       "      <td>1836</td>\n",
       "      <td>5370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>UCD5Lx-3KCYZzCzGF2A60STg</td>\n",
       "      <td>Bowtie Insurance 保泰人壽</td>\n",
       "      <td>51389487</td>\n",
       "      <td>330</td>\n",
       "      <td>80600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>UC8OVLoXv7B1BdOVV44Dz3ig</td>\n",
       "      <td>Project Umbrella</td>\n",
       "      <td>1311132</td>\n",
       "      <td>231</td>\n",
       "      <td>20000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 channel_id                channel_name     views  \\\n",
       "0  UCCjW9xzAsCSKIzDYCx8CuxA                  智偉保險理財Talk   2806950   \n",
       "1  UCxQfqaw1i39eBQG1YJDbDkw                   UTOPIA HK    620097   \n",
       "2  UCFfbH3zDLa47d4nfotQ349Q                       投資最容易   3304042   \n",
       "3  UC7OUGIPx0HIB5HA2OSL-Zhg  MW Insurance Academe 保險為什麼    794169   \n",
       "4  UCD5Lx-3KCYZzCzGF2A60STg       Bowtie Insurance 保泰人壽  51389487   \n",
       "5  UC8OVLoXv7B1BdOVV44Dz3ig            Project Umbrella   1311132   \n",
       "\n",
       "  total_videos subscribers  \n",
       "0          427       39400  \n",
       "1          314        4170  \n",
       "2          394       34900  \n",
       "3         1836        5370  \n",
       "4          330       80600  \n",
       "5          231       20000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved channel stats for 6 channels\n"
     ]
    }
   ],
   "source": [
    "# step 1. identify some well knowned Hong Kong financial/insurance channels\n",
    "channel_ids = [\n",
    "    'UCD5Lx-3KCYZzCzGF2A60STg',  # @Bowtiehongkong\n",
    "    'UC8OVLoXv7B1BdOVV44Dz3ig',  # @projectumbrellahk\n",
    "    'UCCjW9xzAsCSKIzDYCx8CuxA',  # @CW.talkinsurfp\n",
    "    'UC7OUGIPx0HIB5HA2OSL-Zhg',  # @MW31\n",
    "    'UCFfbH3zDLa47d4nfotQ349Q',  # @easy_investment\n",
    "    'UCxQfqaw1i39eBQG1YJDbDkw',  # @utopiahk1406\n",
    "]\n",
    "\n",
    "# step 2. helper function to fetches channel statistics from the YouTube Data API v3\n",
    "def get_channel_stat(youtube, channel_ids):\n",
    "    all_data = []\n",
    "    try:\n",
    "        request = youtube.channels().list(\n",
    "            part=\"snippet,contentDetails,statistics\",\n",
    "            id=\",\".join(channel_ids)\n",
    "        )\n",
    "        response = request.execute()\n",
    "        \n",
    "        for item in response[\"items\"]:\n",
    "            data = {\n",
    "                \"channel_id\": item[\"id\"],\n",
    "                \"channel_name\": item[\"snippet\"][\"title\"],\n",
    "                \"views\": item[\"statistics\"][\"viewCount\"],\n",
    "                \"total_videos\": item[\"statistics\"][\"videoCount\"],\n",
    "                \"subscribers\": item[\"statistics\"][\"subscriberCount\"]\n",
    "            }\n",
    "            all_data.append(data)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching channel stats: {e}\")\n",
    "    \n",
    "    return pd.DataFrame(all_data)\n",
    "\n",
    "# step 3. Fetch channel statistics\n",
    "channel_df = get_channel_stat(youtube, channel_ids)\n",
    "display(channel_df)\n",
    "\n",
    "# step 4. Save the extracted channel statistics to a CSV file\n",
    "BASE_DIR = \"/Users/kevinleungch421/Documents/Profolio Project/Bowie-Youtube-Marketing-Analysis\"\n",
    "os.makedirs(os.path.join(BASE_DIR, \"data/raw\"), exist_ok=True)\n",
    "channel_df.to_csv(os.path.join(BASE_DIR, \"data/raw/youtube_channel_data.csv\"), index=False)\n",
    "print(f\"Saved channel stats for {len(channel_df)} channels\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "309d27b8",
   "metadata": {},
   "source": [
    "## Video data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "e07840b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved 330 videos for channel Bowtie Insurance 保泰人壽\n",
      "Retrieved 231 videos for channel Project Umbrella\n",
      "Retrieved 314 videos for channel UTOPIA HK\n",
      "Retrieved 1650 videos for channel MW Insurance Academe 保險為什麼\n",
      "Retrieved 370 videos for channel 智偉保險理財Talk\n",
      "Retrieved 377 videos for channel 投資最容易\n",
      "\n",
      "Total videos: 3272\n",
      "Columns: ['video_id', 'channel_id', 'channel_name', 'title', 'published_at', 'view_count', 'like_count', 'comment_count', 'duration']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_id</th>\n",
       "      <th>channel_id</th>\n",
       "      <th>channel_name</th>\n",
       "      <th>title</th>\n",
       "      <th>published_at</th>\n",
       "      <th>view_count</th>\n",
       "      <th>like_count</th>\n",
       "      <th>comment_count</th>\n",
       "      <th>duration</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6SvHMLnICzY</td>\n",
       "      <td>UCD5Lx-3KCYZzCzGF2A60STg</td>\n",
       "      <td>Bowtie Insurance 保泰人壽</td>\n",
       "      <td>Coffee Lam 林芊妤 懷孕 8個月 足本專訪 ｜ 產前檢查 疑 染色體異常 流產 定...</td>\n",
       "      <td>2025-10-16T11:16:13Z</td>\n",
       "      <td>17444</td>\n",
       "      <td>496</td>\n",
       "      <td>63</td>\n",
       "      <td>PT1H19M27S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>etfUu9nc-8s</td>\n",
       "      <td>UCD5Lx-3KCYZzCzGF2A60STg</td>\n",
       "      <td>Bowtie Insurance 保泰人壽</td>\n",
       "      <td>心臟科 專科醫生 張仁宇 教你3招 護心秘訣 ｜心臟病 有咩 先兆 ？｜ 通波仔 vs 搭橋...</td>\n",
       "      <td>2025-10-14T11:00:13Z</td>\n",
       "      <td>52601</td>\n",
       "      <td>1672</td>\n",
       "      <td>59</td>\n",
       "      <td>PT1H4M30S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1LpshrYZBVs</td>\n",
       "      <td>UCD5Lx-3KCYZzCzGF2A60STg</td>\n",
       "      <td>Bowtie Insurance 保泰人壽</td>\n",
       "      <td>公務員 跳point 太寬鬆 ？ 政府工 真係 鐵飯碗 ？｜ 曾俊華 Podcast 精華 ...</td>\n",
       "      <td>2025-10-09T11:00:01Z</td>\n",
       "      <td>11560</td>\n",
       "      <td>266</td>\n",
       "      <td>15</td>\n",
       "      <td>PT25M27S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>zdwBZjxL_TI</td>\n",
       "      <td>UCD5Lx-3KCYZzCzGF2A60STg</td>\n",
       "      <td>Bowtie Insurance 保泰人壽</td>\n",
       "      <td>新手爸媽 必睇！ 母嬰健康院 VS 私家診所 BB 預防針 應該點揀？｜ BB打針 注意事項...</td>\n",
       "      <td>2025-10-02T10:30:44Z</td>\n",
       "      <td>285414</td>\n",
       "      <td>74</td>\n",
       "      <td>8</td>\n",
       "      <td>PT11M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hBqz6z6zql4</td>\n",
       "      <td>UCD5Lx-3KCYZzCzGF2A60STg</td>\n",
       "      <td>Bowtie Insurance 保泰人壽</td>\n",
       "      <td>買 自願醫保 必睇！3招教你慎選 VHIS ｜公司醫療保險 墊底費 病房類型 扣稅 成關鍵！...</td>\n",
       "      <td>2025-09-30T10:30:29Z</td>\n",
       "      <td>30291</td>\n",
       "      <td>480</td>\n",
       "      <td>59</td>\n",
       "      <td>PT21M28S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      video_id                channel_id           channel_name  \\\n",
       "0  6SvHMLnICzY  UCD5Lx-3KCYZzCzGF2A60STg  Bowtie Insurance 保泰人壽   \n",
       "1  etfUu9nc-8s  UCD5Lx-3KCYZzCzGF2A60STg  Bowtie Insurance 保泰人壽   \n",
       "2  1LpshrYZBVs  UCD5Lx-3KCYZzCzGF2A60STg  Bowtie Insurance 保泰人壽   \n",
       "3  zdwBZjxL_TI  UCD5Lx-3KCYZzCzGF2A60STg  Bowtie Insurance 保泰人壽   \n",
       "4  hBqz6z6zql4  UCD5Lx-3KCYZzCzGF2A60STg  Bowtie Insurance 保泰人壽   \n",
       "\n",
       "                                               title          published_at  \\\n",
       "0  Coffee Lam 林芊妤 懷孕 8個月 足本專訪 ｜ 產前檢查 疑 染色體異常 流產 定...  2025-10-16T11:16:13Z   \n",
       "1  心臟科 專科醫生 張仁宇 教你3招 護心秘訣 ｜心臟病 有咩 先兆 ？｜ 通波仔 vs 搭橋...  2025-10-14T11:00:13Z   \n",
       "2  公務員 跳point 太寬鬆 ？ 政府工 真係 鐵飯碗 ？｜ 曾俊華 Podcast 精華 ...  2025-10-09T11:00:01Z   \n",
       "3  新手爸媽 必睇！ 母嬰健康院 VS 私家診所 BB 預防針 應該點揀？｜ BB打針 注意事項...  2025-10-02T10:30:44Z   \n",
       "4  買 自願醫保 必睇！3招教你慎選 VHIS ｜公司醫療保險 墊底費 病房類型 扣稅 成關鍵！...  2025-09-30T10:30:29Z   \n",
       "\n",
       "  view_count like_count comment_count    duration  \n",
       "0      17444        496            63  PT1H19M27S  \n",
       "1      52601       1672            59   PT1H4M30S  \n",
       "2      11560        266            15    PT25M27S  \n",
       "3     285414         74             8       PT11M  \n",
       "4      30291        480            59    PT21M28S  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 3272 video details\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Helper functions to fetch video IDs and details\n",
    "def get_video_ids(youtube, playlist_id: str) -> list:\n",
    "    \"\"\"\n",
    "    Fetch video IDs from a channel's uploads playlist.\n",
    "    Returns: List of video IDs\n",
    "    \"\"\"\n",
    "    video_ids = []\n",
    "    next_page_token = None\n",
    "\n",
    "    for attempt in range(3):  # Retry up to 3 times\n",
    "        try:\n",
    "            while True:\n",
    "                request = youtube.playlistItems().list(\n",
    "                    part=\"contentDetails\",\n",
    "                    playlistId=playlist_id,\n",
    "                    maxResults=50,\n",
    "                    pageToken=next_page_token\n",
    "                )\n",
    "                response = request.execute()\n",
    "                video_ids.extend([item['contentDetails']['videoId'] for item in response.get('items', [])])\n",
    "                next_page_token = response.get('nextPageToken')\n",
    "                if not next_page_token:\n",
    "                    break\n",
    "                time.sleep(1.0)  # Delay to avoid rate-limiting\n",
    "            break\n",
    "        except HttpError as e:\n",
    "            print(f\"HTTP Error for playlist {playlist_id}: {e}\")\n",
    "            if e.resp.status == 400:\n",
    "                print(f\"Invalid playlist ID: {playlist_id}\")\n",
    "            elif e.resp.status == 403:\n",
    "                print(\"Possible API quota exceeded.\")\n",
    "            break\n",
    "        except TimeoutError:\n",
    "            print(f\"Timeout for playlist {playlist_id}, retrying ({attempt + 1}/3)...\")\n",
    "            time.sleep(2.0)  # Wait before retrying\n",
    "            continue\n",
    "        except Exception as e:\n",
    "            print(f\"Error for playlist {playlist_id}: {e}\")\n",
    "            break\n",
    "\n",
    "    return video_ids\n",
    "\n",
    "def get_video_details(youtube, video_ids: list, channel_id: str, channel_name: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Fetch details for a list of video IDs.\n",
    "    Returns: DataFrame with video details\n",
    "    \"\"\"\n",
    "    all_video_info = []\n",
    "\n",
    "    for i in range(0, len(video_ids), 50):\n",
    "        for attempt in range(3):  # Retry up to 3 times\n",
    "            try:\n",
    "                request = youtube.videos().list(\n",
    "                    part=\"snippet,contentDetails,statistics\",\n",
    "                    id=','.join(video_ids[i:i+50])\n",
    "                )\n",
    "                response = request.execute()\n",
    "\n",
    "                for video in response.get('items', []):\n",
    "                    stats_to_keep = {\n",
    "                        'snippet': ['title', 'publishedAt'],\n",
    "                        'statistics': ['viewCount', 'likeCount', 'commentCount'],\n",
    "                        'contentDetails': ['duration']\n",
    "                    }\n",
    "                    video_info = {\n",
    "                        'video_id': video['id'],\n",
    "                        'channel_id': channel_id,\n",
    "                        'channel_name': channel_name\n",
    "                    }\n",
    "                    for k in stats_to_keep.keys():\n",
    "                        for v in stats_to_keep[k]:\n",
    "                            key_map = {\n",
    "                                'publishedAt': 'published_at',\n",
    "                                'viewCount': 'view_count',\n",
    "                                'likeCount': 'like_count',\n",
    "                                'commentCount': 'comment_count',\n",
    "                                'title': 'title',\n",
    "                                'duration': 'duration'\n",
    "                            }\n",
    "                            video_info[key_map[v]] = video[k][v] if v in video[k] else None\n",
    "                    all_video_info.append(video_info)\n",
    "                break\n",
    "            except HttpError as e:\n",
    "                print(f\"HTTP Error for video batch {i//50 + 1}, channel {channel_id}: {e}\")\n",
    "                break\n",
    "            except TimeoutError:\n",
    "                print(f\"Timeout for video batch {i//50 + 1}, channel {channel_id}, retrying ({attempt + 1}/3)...\")\n",
    "                time.sleep(2.0)\n",
    "                continue\n",
    "            except Exception as e:\n",
    "                print(f\"Error for video batch {i//50 + 1}, channel {channel_id}: {e}\")\n",
    "                break\n",
    "\n",
    "    return pd.DataFrame(all_video_info)\n",
    "\n",
    "def get_all_channel_videos(youtube, channel_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Fetch video details for all channels.\n",
    "    Returns: DataFrame with video details\n",
    "    \"\"\"\n",
    "    all_videos_df = pd.DataFrame()\n",
    "\n",
    "    channel_df['playlist_id'] = channel_df['channel_id'].apply(lambda x: f\"UU{x[2:]}\")\n",
    "\n",
    "    for _, row in channel_df.iterrows():\n",
    "        channel_id = row['channel_id']\n",
    "        playlist_id = row['playlist_id']\n",
    "        channel_name = row['channel_name']\n",
    "\n",
    "        video_ids = get_video_ids(youtube, playlist_id)\n",
    "        if not video_ids:\n",
    "            print(f\"No videos for channel {channel_name}\")\n",
    "            continue\n",
    "\n",
    "        video_df = get_video_details(youtube, video_ids, channel_id, channel_name)\n",
    "        if not video_df.empty:\n",
    "            print(f\"Retrieved {len(video_df)} videos for channel {channel_name}\")\n",
    "            all_videos_df = pd.concat([all_videos_df, video_df], ignore_index=True)\n",
    "\n",
    "        time.sleep(1.0)  # Delay between channels\n",
    "\n",
    "    if all_videos_df.empty:\n",
    "        print(\"No video data retrieved.\")\n",
    "    else:\n",
    "        print(f\"\\nTotal videos: {len(all_videos_df)}\")\n",
    "        print(f\"Columns: {list(all_videos_df.columns)}\")\n",
    "\n",
    "    return all_videos_df\n",
    "\n",
    "# Step 2: Fetch video details\n",
    "videos_df = get_all_channel_videos(youtube, channel_df)\n",
    "display(videos_df.head())\n",
    "\n",
    "# Step 3: Save to CSV\n",
    "BASE_DIR = \"/Users/kevinleungch421/Documents/Profolio Project/Bowie-Youtube-Marketing-Analysis\"\n",
    "os.makedirs(os.path.join(BASE_DIR, \"data/raw\"), exist_ok=True)\n",
    "videos_df.to_csv(os.path.join(BASE_DIR, \"data/raw/youtube_video_data.csv\"), index=False)\n",
    "print(f\"Saved {len(videos_df)} video details\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "140c70ba",
   "metadata": {},
   "source": [
    "## Comment data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781f14b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed comments for video LLAaPVK93i4\n",
      "Processed comments for video LLAaPVK93i4\n",
      "Processed comments for video LLAaPVK93i4\n",
      "Processed comments for video LLAaPVK93i4\n",
      "Processed comments for video LLAaPVK93i4\n",
      "Processed comments for video LLAaPVK93i4\n",
      "Processed comments for video LLAaPVK93i4\n",
      "Processed comments for video LLAaPVK93i4\n",
      "Processed comments for video LLAaPVK93i4\n",
      "Processed comments for video LLAaPVK93i4\n",
      "Processed comments for video LLAaPVK93i4\n",
      "Processed comments for video LLAaPVK93i4\n",
      "Processed comments for video LLAaPVK93i4\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[127]\u001b[39m\u001b[32m, line 65\u001b[39m\n\u001b[32m     62\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m comments_df\n\u001b[32m     64\u001b[39m \u001b[38;5;66;03m# step 2: Fetch comments\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m65\u001b[39m comments_df = \u001b[43mget_video_comments\u001b[49m\u001b[43m(\u001b[49m\u001b[43myoutube\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvideos_df\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     66\u001b[39m display(comments_df.head())\n\u001b[32m     68\u001b[39m \u001b[38;5;66;03m# step 3: Save to CSV\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[127]\u001b[39m\u001b[32m, line 41\u001b[39m, in \u001b[36mget_video_comments\u001b[39m\u001b[34m(youtube, videos_df)\u001b[39m\n\u001b[32m     39\u001b[39m             \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mProcessed comments for video \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvideo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     40\u001b[39m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m         \u001b[43mtime\u001b[49m\u001b[43m.\u001b[49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1.0\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Delay to avoid rate-limiting\u001b[39;00m\n\u001b[32m     42\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m  \u001b[38;5;66;03m# Exit retry loop on success\u001b[39;00m\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m HttpError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     44\u001b[39m     \u001b[38;5;66;03m#print(f\"HTTP Error for video {video_id}: {e}\")\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# step 1: Helper functions to fetch comments for videos\n",
    "def get_video_comments(youtube, videos_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Fetch comments for all videos in the provided DataFrame.\n",
    "    Returns: DataFrame with comment details\n",
    "    \"\"\"\n",
    "    comments_ls = []\n",
    "\n",
    "    for video_id in videos_df[\"video_id\"].values:\n",
    "        next_page_token = None\n",
    "        for attempt in range(3):  # Retry up to 3 times\n",
    "            try:\n",
    "                while True:\n",
    "                    # Fetch comments using YouTube CommentThreads API\n",
    "                    comments_data = youtube.commentThreads().list(\n",
    "                        part=\"snippet\",\n",
    "                        videoId=video_id,\n",
    "                        maxResults=100,\n",
    "                        pageToken=next_page_token\n",
    "                    ).execute()\n",
    "\n",
    "                    # Process each comment\n",
    "                    for comment in comments_data.get(\"items\", []):\n",
    "                        comment_dict = {\n",
    "                            \"comment_id\": comment[\"snippet\"][\"topLevelComment\"][\"id\"],\n",
    "                            \"video_id\": comment[\"snippet\"][\"topLevelComment\"][\"snippet\"][\"videoId\"],\n",
    "                            \"channel_id\": comment[\"snippet\"][\"topLevelComment\"][\"snippet\"][\"channelId\"],\n",
    "                            \"comment_text\": comment[\"snippet\"][\"topLevelComment\"][\"snippet\"][\"textOriginal\"],\n",
    "                            \"published_at\": datetime.strptime(\n",
    "                                comment[\"snippet\"][\"topLevelComment\"][\"snippet\"][\"publishedAt\"],\n",
    "                                \"%Y-%m-%dT%H:%M:%SZ\"\n",
    "                            )\n",
    "                        }\n",
    "                        comments_ls.append(comment_dict)\n",
    "\n",
    "                    # Get next page token\n",
    "                    next_page_token = comments_data.get(\"nextPageToken\")\n",
    "                    if not next_page_token:\n",
    "                        print(f\"Processed comments for video {video_id}\")\n",
    "                        break\n",
    "                    time.sleep(1.0)  # Delay to avoid rate-limiting\n",
    "                break  # Exit retry loop on success\n",
    "            except HttpError as e:\n",
    "                print(f\"HTTP Error for video {video_id}: {e}\")\n",
    "                if e.resp.status == 403:\n",
    "                    print(f\"Comments disabled for video {video_id}\")\n",
    "                break\n",
    "            except TimeoutError:\n",
    "                print(f\"Timeout for video {video_id}, retrying ({attempt + 1}/3)...\")\n",
    "                time.sleep(2.0)\n",
    "                continue\n",
    "            except Exception as e:\n",
    "                print(f\"Error for video {video_id}: {e}\")\n",
    "                break\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    comments_df = pd.DataFrame(comments_ls)\n",
    "\n",
    "    # Drop empty comments and duplicates\n",
    "    comments_df = comments_df[comments_df[\"comment_text\"] != \"\"].drop_duplicates()\n",
    "\n",
    "    return comments_df\n",
    "\n",
    "# step 2: Fetch comments\n",
    "comments_df = get_video_comments(youtube, videos_df)\n",
    "display(comments_df.head())\n",
    "\n",
    "# step 3: Save to CSV\n",
    "BASE_DIR = \"/Users/kevinleungch421/Documents/Profolio Project/Bowie-Youtube-Marketing-Analysis\"\n",
    "os.makedirs(os.path.join(BASE_DIR, \"data/raw\"), exist_ok=True)\n",
    "comments_df.to_csv(os.path.join(BASE_DIR, \"data/raw/youtube_comments_data.csv\"), index=False)\n",
    "print(f\"Saved {len(comments_df)} comments\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a86755fa",
   "metadata": {},
   "source": [
    "# small youtuber"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08ee246",
   "metadata": {},
   "source": [
    "@projectumbrellahk\n",
    "UC8OVLoXv7B1BdOVV44Dz3ig\n",
    "\n",
    "@CW.talkinsurfp\n",
    "UCCjW9xzAsCSKIzDYCx8CuxA\n",
    "\n",
    "Adriana的保險實戰攻略\n",
    "@adrianaszeyu\n",
    "UCn2mY9NLMvu7oJuLbFC-kIw\n",
    "\n",
    "UncleWill\n",
    "@unclewill894\n",
    "UCuv-es1NKE9mxFU3PTv3zkg\n",
    "\n",
    "MW Insurance Academe 保險為什麼\n",
    "@MW31\n",
    "UC7OUGIPx0HIB5HA2OSL-Zhg\n",
    "\n",
    "10Life 保險比較平台\n",
    "@10LifeHK\n",
    "UCz8b7EYrOF4iXFIsap30kkw\n",
    "\n",
    "Blue Insurance Hong Kong\n",
    "@BlueHKinsurance\n",
    "UCXqmN9Z56cX2VPXZ-ZPnS1A\n",
    "\n",
    "投資最容易\n",
    "@easy_investment\n",
    "UCFfbH3zDLa47d4nfotQ349Q\n",
    "\n",
    "UTOPIA HK\n",
    "@utopiahk1406\n",
    "UCxQfqaw1i39eBQG1YJDbDkw\n",
    "\n",
    "Adrian Lee\n",
    "@adrianlee-9036\n",
    "UCPO68WX6rtspcv-kmgK2ufQ\n",
    "\n",
    "大佬Kirk保險日記\n",
    "@kirk2677\n",
    "UCOr4rh-QXaVY_ZQzTqKesiQ\n",
    "\n",
    "王傲山MarcusWong綜合頻道\n",
    "@AIARoundTableFamily\n",
    "UCLblmEwmgBr-UCxP9ZsbUnA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55db0fb1",
   "metadata": {},
   "source": [
    "# Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4846481f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    4767\n",
       "1    3870\n",
       "2    1527\n",
       "3     660\n",
       "4    1288\n",
       "Name: duration, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Function to convert the YouTube video duration from ISO 8601 format (str) to seconds (int)\n",
    "def convert_iso8601_duration(duration):\n",
    "    \"\"\"\n",
    "    Convert YouTube ISO 8601 duration (e.g., 'PT4M36S') to seconds.\n",
    "    Returns: Duration in seconds, or input if already numeric\n",
    "    \"\"\"\n",
    "    # If duration is numeric (float or int) or NaN, return as-is or 0\n",
    "    if isinstance(duration, (int, float)) and not np.isnan(duration):\n",
    "        return int(duration)\n",
    "    if not duration or pd.isna(duration):\n",
    "        return 0\n",
    "    \n",
    "    # Process ISO 8601 string\n",
    "    time_extractor = re.compile(r'PT(?:(\\d+)H)?(?:(\\d+)M)?(?:(\\d+)S)?')\n",
    "    extracted = time_extractor.match(str(duration))\n",
    "    if extracted:\n",
    "        hours = int(extracted.group(1)) if extracted.group(1) else 0\n",
    "        minutes = int(extracted.group(2)) if extracted.group(2) else 0\n",
    "        seconds = int(extracted.group(3)) if extracted.group(3) else 0\n",
    "        return hours * 3600 + minutes * 60 + seconds\n",
    "    return 0\n",
    "\n",
    "# Load CSV file into pandas DataFrame\n",
    "videos_df = pd.read_csv(\"/Users/kevinleungch421/Documents/Profolio Project/Bowie-Youtube-Marketing-Analysis/data/raw/youtube_video_data.csv\")\n",
    "\n",
    "# Convert video duration in pandas DataFrame\n",
    "videos_df[\"duration\"] = videos_df[\"duration\"].apply(convert_iso8601_duration)\n",
    "display(videos_df.duration.head())\n",
    "\n",
    "# Save CSV file with updated durations\n",
    "os.makedirs(os.path.join(BASE_DIR, \"data/processed\"), exist_ok=True)\n",
    "videos_df.to_csv(os.path.join(BASE_DIR, \"data/processed/youtube_video_data.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "7fa5a5bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading vader_lexicon: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:1032)>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import nltk\n",
    "\n",
    "# Download VADER lexicon\n",
    "nltk.download('vader_lexicon', quiet=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b863d0d0",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "277f49c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed sentiment for 17790 comments\n",
      "Saved 17790 comments to /Users/kevinleungch421/Documents/Profolio Project/Bowie-Youtube-Marketing-Analysis/data/processed/youtube_comments_data.csv\n"
     ]
    }
   ],
   "source": [
    "def analyze_comments_sentiment(comments_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Apply VADER sentiment analysis to comments DataFrame.\"\"\"\n",
    "    # Initialize VADER\n",
    "    vader_sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "    # Sentiment scoring\n",
    "    def get_sentiment(text):\n",
    "        return vader_sia.polarity_scores(text)[\"compound\"] if isinstance(text, str) else 0.0\n",
    "\n",
    "    # Sentiment categorization\n",
    "    def categorize_sentiment(score):\n",
    "        if score > 0.05:\n",
    "            return \"positive\"\n",
    "        elif score < -0.05:\n",
    "            return \"negative\"\n",
    "        return \"neutral\"\n",
    "\n",
    "    # Apply sentiment analysis if DataFrame is not empty\n",
    "    if not comments_df.empty:\n",
    "        comments_df[\"vader_score\"] = comments_df[\"comment_text\"].apply(get_sentiment)\n",
    "        comments_df[\"vader_sentiment\"] = comments_df[\"vader_score\"].apply(categorize_sentiment)\n",
    "        print(f\"Processed sentiment for {len(comments_df)} comments\")\n",
    "    else:\n",
    "        print(\"No comments to process.\")\n",
    "        # Ensure columns exist in empty DataFrame\n",
    "        comments_df = comments_df.reindex(columns=[\n",
    "            \"comment_id\", \"video_id\", \"channel_id\", \"comment_text\", \"published_at\",\n",
    "            \"vader_score\", \"vader_sentiment\"\n",
    "        ])\n",
    "\n",
    "    return comments_df\n",
    "\n",
    "# Load comments CSV\n",
    "comments_df = pd.read_csv(\"/Users/kevinleungch421/Documents/Profolio Project/Bowie-Youtube-Marketing-Analysis/data/raw/youtube_comments_data.csv\")\n",
    "\n",
    "# Apply sentiment analysis\n",
    "comments_df = analyze_comments_sentiment(comments_df)\n",
    "\n",
    "# Save to processed directory\n",
    "os.makedirs(os.path.join(BASE_DIR, \"data/processed\"), exist_ok=True)\n",
    "OUTPUT_CSV = os.path.join(BASE_DIR, \"data/processed/youtube_comments_data.csv\")\n",
    "comments_df.to_csv(OUTPUT_CSV, index=False)\n",
    "print(f\"Saved {len(comments_df)} comments to {OUTPUT_CSV}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "6b922e21",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mysql_user' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[125]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      4\u001b[39m comments_df = pd.read_csv(\u001b[33m\"\u001b[39m\u001b[33m/Users/kevinleungch421/Documents/Profolio Project/Bowie-Youtube-Marketing-Analysis/data/processed/youtube_comments_data.csv\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Connect to local MySQL database\u001b[39;00m\n\u001b[32m      7\u001b[39m connection = mysql.connector.connect(\n\u001b[32m      8\u001b[39m     host = \u001b[33m\"\u001b[39m\u001b[33mlocalhost\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      9\u001b[39m     port = \u001b[32m3306\u001b[39m,\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m     user = \u001b[43mmysql_user\u001b[49m,\n\u001b[32m     11\u001b[39m     password = mysql_password,\n\u001b[32m     12\u001b[39m     database = \u001b[33m\"\u001b[39m\u001b[33myoutube_analytics\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     13\u001b[39m )\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# Create a cursor object to execute SQL queries\u001b[39;00m\n\u001b[32m     16\u001b[39m cursor = connection.cursor()\n",
      "\u001b[31mNameError\u001b[39m: name 'mysql_user' is not defined"
     ]
    }
   ],
   "source": [
    "# Load CSV files into pandas DataFrames\n",
    "channel_df = pd.read_csv(\"/Users/kevinleungch421/Documents/Profolio Project/Bowie-Youtube-Marketing-Analysis/data/processed/youtube_channel_data.csv\")\n",
    "videos_df = pd.read_csv(\"/Users/kevinleungch421/Documents/Profolio Project/Bowie-Youtube-Marketing-Analysis/data/processed/youtube_video_data.csv\")\n",
    "comments_df = pd.read_csv(\"/Users/kevinleungch421/Documents/Profolio Project/Bowie-Youtube-Marketing-Analysis/data/processed/youtube_comments_data.csv\")\n",
    "\n",
    "# Connect to local MySQL database\n",
    "connection = mysql.connector.connect(\n",
    "    host = \"localhost\",\n",
    "    port = 3306,\n",
    "    user = mysql_user,\n",
    "    password = mysql_password,\n",
    "    database = \"youtube_analytics\"\n",
    ")\n",
    "\n",
    "# Create a cursor object to execute SQL queries\n",
    "cursor = connection.cursor()\n",
    "\n",
    "# Drop existing MySQL tables \n",
    "tables_to_drop = [\"comments\", \"videos\", \"channels\"]\n",
    "for table in tables_to_drop:\n",
    "    cursor.execute(f\"DROP TABLE IF EXISTS {table};\")\n",
    "        \n",
    "try:\n",
    "    # Create an SQLAlchemy engine for interacting with the MySQL database\n",
    "    engine = create_engine(f\"mysql+mysqlconnector://{mysql_user}:{mysql_password}@localhost:3306/youtube_analytics\") \n",
    "    \n",
    "    # Load the YouTube channels DataFrame into the MySQL channels table\n",
    "    try:\n",
    "        channel_df.to_sql(\"channels\", con=engine, if_exists=\"replace\", index=False)\n",
    "        print(\"Channels data successfully loaded into local MySQL database.\")\n",
    "    except Exception as e:\n",
    "        print(\"Error loading channels data:\", e)\n",
    "    \n",
    "    # Load the YouTube videos DataFrame into the MySQL videos table\n",
    "    try:\n",
    "        videos_df.to_sql(\"videos\", con=engine, if_exists=\"replace\", index=False)\n",
    "        print(\"Videos data successfully loaded into local MySQL database.\")\n",
    "    except Exception as e:\n",
    "        print(\"Error loading videos data:\", e)\n",
    "    \n",
    "    # Load the YouTube comments DataFrame into the MySQL comments table\n",
    "    try:\n",
    "        comments_df.to_sql(\"comments\", con=engine, if_exists=\"replace\", index=False)\n",
    "        print(\"Comments data successfully loaded into local MySQL database.\")\n",
    "    except Exception as e:\n",
    "        print(\"Error loading comments data:\", e)\n",
    "    \n",
    "except Exception as e:\n",
    "    # Print error if exception occurs when connecting to the database \n",
    "    print(\"Error connecting to local MySQL database:\", e)\n",
    "\n",
    "finally:\n",
    "    # Close the cursor and connection to free up resources\n",
    "    cursor.close()\n",
    "    connection.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0110885",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
